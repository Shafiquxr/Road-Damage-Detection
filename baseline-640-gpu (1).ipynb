{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14294607,"sourceType":"datasetVersion","datasetId":9124799},{"sourceId":291317913,"sourceType":"kernelVersion"}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys, subprocess\n\n# Remove integrations that caused crashes in this chat\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"-q\", \"uninstall\", \"-y\", \"ray\", \"wandb\", \"tensorboard\"], check=False)\n\n# Ultralytics is usually preinstalled on Kaggle, but ensure it's importable\ntry:\n    import ultralytics  # noqa\nexcept Exception:\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \"ultralytics==8.2.0\", \"--no-deps\"], check=True)\n\n# OpenCV headless (safe; no deps)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \"opencv-python-headless==4.10.0.84\", \"--no-deps\"], check=False)\n\nimport ultralytics\nprint(\"✅ ultralytics:\", ultralytics.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, sys, importlib\nfrom pathlib import Path\n\n# Disable W&B (prevents project-name crashes)\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nos.environ[\"WANDB_SILENT\"] = \"true\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\nWORK = \"/kaggle/working\"\n\n# Ensure sitecustomize can be found in this process and DDP subprocesses\nif WORK not in sys.path:\n    sys.path.insert(0, WORK)\nos.environ[\"PYTHONPATH\"] = WORK + \":\" + os.environ.get(\"PYTHONPATH\", \"\")\n\n# --- Write sitecustomize to patch torch.load inside every subprocess ---\nPath(f\"{WORK}/sitecustomize.py\").write_text(r'''\nimport os\nos.environ.setdefault(\"WANDB_DISABLED\", \"true\")\nos.environ.setdefault(\"WANDB_MODE\", \"disabled\")\nos.environ.setdefault(\"WANDB_SILENT\", \"true\")\nos.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n\ntry:\n    import torch\n\n    # Patch only once\n    if not getattr(torch, \"_weights_only_off\", False):\n        _orig_torch_load = torch.load\n\n        def patched_torch_load(*args, **kwargs):\n            # Force full load in PyTorch 2.6+\n            kwargs.setdefault(\"weights_only\", False)\n            try:\n                return _orig_torch_load(*args, **kwargs)\n            except TypeError as e:\n                # If some torch build doesn't accept weights_only kwarg, retry without it\n                if \"weights_only\" in str(e):\n                    kwargs.pop(\"weights_only\", None)\n                    return _orig_torch_load(*args, **kwargs)\n                raise\n\n        torch.load = patched_torch_load\n        torch._weights_only_off = True\nexcept Exception:\n    pass\n''')\n\n# Force reload sitecustomize in current notebook process\nsys.modules.pop(\"sitecustomize\", None)\nimportlib.invalidate_caches()\nimport sitecustomize  # noqa: F401\n\n# Patch torch.load ALSO in current process (belt + suspenders)\nimport torch\nif not getattr(torch, \"_weights_only_off\", False):\n    _orig = torch.load\n    def patched_torch_load(*args, **kwargs):\n        kwargs.setdefault(\"weights_only\", False)\n        try:\n            return _orig(*args, **kwargs)\n        except TypeError as e:\n            if \"weights_only\" in str(e):\n                kwargs.pop(\"weights_only\", None)\n                return _orig(*args, **kwargs)\n            raise\n    torch.load = patched_torch_load\n    torch._weights_only_off = True\n\nprint(\"✅ torch:\", torch.__version__)\nprint(\"✅ torch.load name:\", torch.load.__name__)\n\n# Verify subprocess inherits patch (DDP uses subprocesses)\nimport subprocess as sp\nimport sys as _sys\nout = sp.check_output([_sys.executable, \"-c\", \"import torch; print(torch.load.__name__)\"]).decode().strip()\nprint(\"✅ subprocess torch.load name:\", out)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom ultralytics import YOLO\n\nprint(\"cuda:\", torch.cuda.is_available(), \"count:\", torch.cuda.device_count())\nDEVICE = \"0,1\" if torch.cuda.is_available() and torch.cuda.device_count() >= 2 else (0 if torch.cuda.is_available() else \"cpu\")\nprint(\"✅ DEVICE:\", DEVICE)\n\n_ = YOLO(\"yolov8l.pt\")\nprint(\"✅ YOLO('yolov8l.pt') loads OK (UnpicklingError fixed)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nimport os, math\nfrom collections import Counter\n\nINPUT = Path(\"/kaggle/input\")\ncands = list(INPUT.glob(\"**/randomized_dataset\"))\nassert cands, \"❌ Could not find 'randomized_dataset' under /kaggle/input\"\nDATA_ROOT = cands[0]\nprint(\"✅ DATA_ROOT:\", DATA_ROOT)\n\nIMG_EXTS = (\".jpg\",\".jpeg\",\".png\",\".JPG\",\".JPEG\",\".PNG\")\n\ndef has_images(d: Path) -> bool:\n    return d.exists() and any(p.is_file() and p.suffix in IMG_EXTS for p in d.iterdir())\n\ndef find_split_dirs(root: Path, split: str):\n    candidates = [\n        (root/split/\"images\", root/split/\"labels\"),\n        (root/\"images\"/split, root/\"labels\"/split),\n        (root/split, root/split/\"labels\"),\n    ]\n    for img_dir, lbl_dir in candidates:\n        if has_images(img_dir):\n            return img_dir, (lbl_dir if lbl_dir.exists() else None)\n    return None, None\n\nOUT = Path(\"/kaggle/working/rdd_clean\")\n\ndef symlink_dir_or_files(src: Path, dst: Path):\n    dst.parent.mkdir(parents=True, exist_ok=True)\n    if dst.exists() or dst.is_symlink():\n        return\n    try:\n        os.symlink(src, dst, target_is_directory=True)\n    except Exception:\n        dst.mkdir(parents=True, exist_ok=True)\n        for f in src.iterdir():\n            if f.is_file() and f.suffix in IMG_EXTS:\n                try:\n                    os.symlink(f, dst/f.name)\n                except:\n                    pass\n\ndef clamp01(x): \n    return 0.0 if x < 0 else 1.0 if x > 1 else x\n\ndef clean_labels(src_img: Path, src_lbl: Path, out_lbl: Path):\n    out_lbl.mkdir(parents=True, exist_ok=True)\n    stats = Counter()\n    imgs = [p for p in src_img.iterdir() if p.is_file() and p.suffix in IMG_EXTS]\n\n    for img_path in imgs:\n        src_txt = src_lbl / f\"{img_path.stem}.txt\"\n        out_txt = out_lbl / f\"{img_path.stem}.txt\"\n\n        if not src_txt.exists():\n            out_txt.write_text(\"\")\n            stats[\"missing_label_created\"] += 1\n            continue\n\n        new_lines = []\n        for line in src_txt.read_text().strip().splitlines():\n            parts = line.strip().split()\n            if len(parts) != 5:\n                stats[\"bad_format\"] += 1\n                continue\n            try:\n                cls = int(float(parts[0]))\n                xc, yc, w, h = map(float, parts[1:])\n            except:\n                stats[\"non_numeric\"] += 1\n                continue\n\n            if cls < 0 or cls > 4:\n                stats[\"bad_class\"] += 1\n                continue\n            if w <= 0 or h <= 0:\n                stats[\"bad_wh\"] += 1\n                continue\n\n            xc, yc, w, h = map(clamp01, [xc, yc, w, h])\n            new_lines.append(f\"{cls} {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\")\n\n        out_txt.write_text(\"\\n\".join(new_lines) + (\"\\n\" if new_lines else \"\"))\n\n    return stats\n\nsummary = Counter()\nfor split in [\"train\",\"val\",\"test\"]:\n    src_img, src_lbl = find_split_dirs(DATA_ROOT, split)\n    assert src_img is not None, f\"❌ No images found for split={split}\"\n\n    symlink_dir_or_files(src_img, OUT / \"images\" / split)\n\n    if split in [\"train\",\"val\"]:\n        assert src_lbl is not None, f\"❌ No labels found for split={split}\"\n        summary.update(clean_labels(src_img, src_lbl, OUT / \"labels\" / split))\n\nprint(\"✅ label cleaning summary:\", dict(summary.most_common(10)))\n\nyaml_path = Path(\"/kaggle/working/rdd_clean.yaml\")\nyaml_path.write_text(\"\"\"\npath: /kaggle/working/rdd_clean\ntrain: images/train\nval: images/val\ntest: images/test\n\nnames:\n  0: longitudinal_crack\n  1: transverse_crack\n  2: alligator_crack\n  3: other_corruption\n  4: pothole\n\"\"\".strip() + \"\\n\")\nprint(\"✅ YAML:\", yaml_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time, torch\nfrom ultralytics import YOLO\n\nDEVICE = \"0,1\" if torch.cuda.is_available() and torch.cuda.device_count() >= 2 else (0 if torch.cuda.is_available() else \"cpu\")\nprint(\"✅ DEVICE:\", DEVICE)\n\nSTART = time.time()\nTIME_LIMIT_SEC = 11.3 * 3600  # stop before Kaggle runtime kills session\n\ndef epoch_trend_and_stop(trainer):\n    # print only on rank0 (DDP)\n    rank = getattr(trainer, \"rank\", 0)\n    if rank in (-1, 0):\n        m = getattr(trainer, \"metrics\", {})\n        if isinstance(m, dict):\n            print(f\"\\n[EPOCH {trainer.epoch+1}] mAP50={m.get('metrics/mAP50(B)')} | mAP50-95={m.get('metrics/mAP50-95(B)')}\")\n    if time.time() - START > TIME_LIMIT_SEC:\n        print(\"\\n⏱️ Time limit reached → stopping safely.\")\n        trainer.stop = True\n\nmodel = YOLO(\"yolov8l.pt\")\nmodel.callbacks.setdefault(\"on_fit_epoch_end\", [])\nmodel.callbacks[\"on_fit_epoch_end\"].append(epoch_trend_and_stop)\n\n# total batch for 2xT4\nBATCH = 32  # if OOM -> 24 -> 16\n\nmodel.train(\n    data=str(yaml_path),\n    imgsz=640,\n    epochs=500,              # time stop ends it\n    batch=BATCH,\n    device=DEVICE,\n    workers=2,               # if RAM crash -> 0\n    cache=False,             # do NOT use True (RAM crash)\n    amp=True,\n    verbose=True,\n    project=\"runs\",\n    name=\"baseline_640_2gpu\",\n    patience=25,\n    save=True,\n    save_period=5,\n\n    optimizer=\"SGD\",\n    lr0=0.01,\n    lrf=0.01,\n    momentum=0.937,\n    weight_decay=0.0005,\n\n    mosaic=0.8,\n    mixup=0.10,\n    translate=0.10,\n    scale=0.50,\n    fliplr=0.5,\n    close_mosaic=10\n)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-12T14:58:14.349Z"}},"outputs":[],"execution_count":null}]}